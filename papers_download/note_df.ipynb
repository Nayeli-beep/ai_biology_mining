from Bio import Entrez
import pandas as pd
import time
import json


Entrez.email = "youremail@example.com"

ai_term = ["Artificial Intelligence", "Machine learning", "Deep learning"]
bio_term =["Biology", "Neurobiology", "Bioinformatics", "Genetics", "Ecology"]

dict_terms={}

for i in bio_term:
  tms[i]=ai_term

#Build search search terms requests 
def pubmed_data(terms, start_year="2020", end_year="2025", max_res=3):
    sugg = f""""{terms[0]}"[Title/Abstract] AND "{terms[1]}"[Title/Abstract] AND ({start_year}:{end_year}[dp])"""
    handle = Entrez.esearch(db="pubmed", term=sugg, retmax=max_res)
    results = Entrez.read(handle)
    handle.close()
    return results.get("IdList",[])

#Data extraction/DataFrame construction 
def details_rec(id_list, ai_term, bio_term): 
  paper_details=[]
  df = pd.DataFrame(columns=['PMID', 'Title', 'Abstract', 'Year', 'MeSH_Keywords', 'Manual_Keywords', 'URL', 'Term_List','Bio_Term])

  for pmid in id_list:
      try:
          handle = Entrez.efetch(db="pubmed", id=pmid, retmode="xml")
          docs = Entrez.read(handle)

      except Entrez.Parser.CorruptedXMLError as e:
          print(f"Error parsing XML for PMID {pmid}: {e}")
          continue

  #JSON requests
  #print(json.dumps(docs, indent=4, default=str))

  for doc in docs['PubmedArticle]:
      paper = doc['MedlineCitation']['Article']
  
      title = paper['ArticleTitle']
      abstract = ' '.join(paper['Abstract']['AbstractText']) if 'Abstract' in paper and 'AbstractText' in paper['Abstract'] else 'Abstract not avaible'
      year = paper['Journal']['JournalIssue']['PubDate'].get('Year', 'No year found')
      keywords = ','.join(keyword['DescriptorName'] for keyword in doc['MedlineCitation'].get('MeshHeadingList', [])) or 'Keywords Not Avaible'

      KWD_list_compl = ["genomics", "data analysis","Predictive modeling","neural network", "algorithm", "deep learning", "artificial intelligence", "machine learning", "reinforcement learning", "biology"]
      KWD_list = []

      KWD_list_str="Keyword not found"

      for keyss in KWD_list_compl:
        if keyss.lower() in title.lower() or keyss.lower() in abstract.lower():
          KWD_list.append(keyss)

      if KWD_list:
        KWD_list_str = ','.join(KWD_list)

      url = f"https://www.ncbi.nlm.nih.gov/pubmed/{pmid}"

      pubmed_df=pd.DataFrame({
                'PMID':[pmid],
                'Title':[title],
                'Abstract':[abstract],
                'Year':[year],
                'MeSH_Keywords': [keywords],
                'Manual_Keywords': [KWD_list_str],
                'URL': [url],
                'Term_List':[ai_term],
                'Bio_Term':[bio_term]})

      df = pd.concat([df,pubmed_df], ignore_index=True)

  time.sleep(1)
  handle.close()

return df

#Iterate search terms/combines data frames/clean PMID duplicates
all_results_df = []
for bioterm, termlist in tms.items():
  for ai_term in termlist:
    id_list = pubmed_data([bioterm, ai_term], start_year="2000", end_year="2025")
    time.sleep(1)
    df_result = details_rec(id_list, [ai_term], bioterm)
    all_results_df.append(df_result)

if all_results_df:
    all_results_df = pd.concat(all_results_df, ignore_index=True)
    all_results_df = all_results_df.drop_duplicates(subset=['PMID'])

#Counting PMIDs like sheep 
    pmidcount=0
    for pmid in all_results_df['PMID']:
      wordsk = pmid.split(', ')
      countk = len(wordsk)
      pmidcount += (countk)

print(f"\nPMID found: {pmidcount}")
