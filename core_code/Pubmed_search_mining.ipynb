{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Crecimiento de la Inteligencia Artificial en la Ciencias Biológicas (2000-2025): Búsquedas semiautomatizadas en PubMed**"
      ],
      "metadata": {
        "id": "1JcDpRMfWs9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalación de bibliotecas"
      ],
      "metadata": {
        "id": "Dnww2kxOUAHZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd-xWs4hT2Bn"
      },
      "outputs": [],
      "source": [
        "!pip install biopython pandas wordcloud matplotlib numpy seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agrega los requisitos necesarios para conectarse a la base de datos de NCBI, para comenzar con el análisis semántico y la generación de gráficas"
      ],
      "metadata": {
        "id": "ALZrq8C_UZR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez\n",
        "import pandas as pd\n",
        "import time\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re"
      ],
      "metadata": {
        "id": "IbeVzC-xeZ7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definición los términos de búsqueda**\n",
        "\n",
        "Escribe tu email para usar la API de PubMed. Crea dos listas con los términos de búsqueda de la IA y los campos de búsqueda. Agrupalos en un diccionario para facilitar su búsqueda"
      ],
      "metadata": {
        "id": "cMiZbkspelJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Entrez.email = \"youremail@example.com\"\n",
        "\n",
        "ai_field = [\"Artificial Intelligence\", \"Machine learning\", \"Deep learning\"]\n",
        "biofield = [\"Biology\", \"Biomedicine\", \"Bioinformatics\", \"Genetics\", \"Ecology\"]\n",
        "\n",
        "dict_terms = {}\n",
        "for i in biofield:\n",
        "    dict_terms[i] = ai_field\n",
        "\n",
        "print(dict_terms)"
      ],
      "metadata": {
        "id": "kJbNDGUNUXDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Función 1. Estrategia de búsqueda**\n",
        "\n",
        "Se toman dos términos siendo uno de biología y otro de IA, los cuales se buscan en los artículos de PubMed, donde contegan ambos términos. Estos se limitan a un rango de 500 artículos encontrados entre 2000-2025. Los resultados obtenidos se regresan en una lista de PMIDs.\n",
        "\n",
        "\n",
        "Recursos:\n",
        "\n",
        "(1) Dennstädt et al. 2024\n",
        "\n",
        "(2) Stack Overflow (2023)"
      ],
      "metadata": {
        "id": "teNvE8v8UzCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Realiza búsquedas en PubMed por combinación de términos de IA +  biología encontrados en el título y resumen\n",
        "def pubmed_data(terms, start_year=\"2000\", end_year=\"2025\", max_res=500):\n",
        "  sugg = f\"\"\"\"{terms[0]}\"[Title/Abstract] AND \"{terms[1]}\"[Title/Abstract] AND ({start_year}:{end_year}[dp])\"\"\"\n",
        "  handle = Entrez.esearch(db=\"pubmed\", term=sugg, retmax=max_res)\n",
        "  results = Entrez.read(handle)\n",
        "  handle.close()\n",
        "  return results.get(\"IdList\",[])"
      ],
      "metadata": {
        "id": "P-5YpeByUsH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Función 2. Minería de texto**\n",
        "\n",
        "Por medio del módulo 'Entrez.efetch' es posible obtener los registros completos de cada documento de la lista de PMIDs. Se obtiene un dataframe donde se agregaron los metadatos solicitados.\n",
        "\n",
        "\n",
        "Recurso:\n",
        "\n",
        "(3) TLDWTutorials. (2024)"
      ],
      "metadata": {
        "id": "Z7K2Ag_cVYcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def details_rec(id_list, ai_term, bio_term):\n",
        "    df = pd.DataFrame(columns=['PMID', 'Title', 'Abstract', 'Year', 'MeSH_Keywords', 'URL', 'Term_List','Bio_Term'])\n",
        "\n",
        "     #Complete XML\n",
        "        #print(json.dumps(docs, indent=4, default=str))\n",
        "\n",
        "    for pmid in id_list:\n",
        "        try:\n",
        "            handle = Entrez.efetch(db=\"pubmed\", id=pmid, retmode=\"xml\")\n",
        "            docs = Entrez.read(handle)\n",
        "            handle.close()\n",
        "\n",
        "            for doc in docs['PubmedArticle']:\n",
        "                paper = doc['MedlineCitation']['Article']\n",
        "\n",
        "                title = paper['ArticleTitle']\n",
        "                abstract = ' '.join(paper['Abstract']['AbstractText']) if 'Abstract' in paper and 'AbstractText' in paper['Abstract'] else 'Abstract not available'\n",
        "                year = paper['Journal']['JournalIssue']['PubDate'].get('Year', 'No year found')\n",
        "                #Palabras clave MeSH\n",
        "                keywords = ', '.join(keyword['DescriptorName'] for keyword in doc['MedlineCitation'].get ('MeshHeadingList', [])) or 'Keyword not found'\n",
        "\n",
        "                url = f\"https://www.ncbi.nlm.nih.gov/pubmed/{pmid}\"\n",
        "\n",
        "                pubmed_df=pd.DataFrame({\n",
        "                    'PMID':[pmid],\n",
        "                    'Title':[title],\n",
        "                    'Abstract':[abstract],\n",
        "                    'Year':[year],\n",
        "                    'MeSH_Keywords': [keywords],\n",
        "                    'URL': [url],\n",
        "                    'Term_List': [ai_term],\n",
        "                    'Bio_Term':[bio_term]})\n",
        "\n",
        "                df = pd.concat([df,pubmed_df], ignore_index=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"PMID error {pmid}: {e}\")\n",
        "            continue\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "J29cVATaVS6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procesamiento de los datos**\n",
        "\n",
        "Se itera cada combinación de los términos de IA y campo de la biología. Se extraen los metadatos de cada artículo y se almacenan en dataframes separados, que posteriormente son concatenados a un dataframe único  \n",
        "\n",
        "\n",
        "Recursos:\n",
        "\n",
        "(4) Stack Overflow. (2015)"
      ],
      "metadata": {
        "id": "Lt4UR4EIWIxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_results_df = []\n",
        "for bioterm, termlist in dict_terms.items():\n",
        "  for ai_term in termlist:\n",
        "    id_list = pubmed_data([bioterm, ai_term], start_year=\"2000\", end_year=\"2025\")\n",
        "    time.sleep(2)\n",
        "    df_result = details_rec(id_list, ai_term, bioterm)\n",
        "    all_results_df.append(df_result)\n",
        "\n",
        "if all_results_df:\n",
        "    #Se verifica si hay resultados existentes. Si los hay, se agregan al nuevo dataframe\n",
        "    df_final = pd.concat(all_results_df, ignore_index=True)\n",
        "    print(df_final)\n",
        "\n",
        "    #Este paso es para revisar el número total de PMIDs encontrados de todas las combinaciones\n",
        "    pmid_count = len(df_final)\n",
        "\n",
        "else:\n",
        "    pmid_count=0\n",
        "    print(\"No results found\")\n",
        "print(f\"\\nPMID found: {pmid_count}\")"
      ],
      "metadata": {
        "id": "iyoWTnVlV_VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Extra*\n",
        "\n",
        "**Almacenamiento**\n",
        "\n",
        "Se guarda el dataframe final sin cambios en un archivo csv"
      ],
      "metadata": {
        "id": "ayIEkztfgfJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Se almacena y se vuelve a leer el dataframe con los datos en crudo\n",
        "df_final.to_csv('df_final_raw_150525.csv', index=False)\n",
        "df_final_raw_150525 = pd.read_csv('df_final_raw_150525.csv')"
      ],
      "metadata": {
        "id": "255ZEfiHgr6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elimínalos**\n",
        "\n",
        "Se remueven los PMIDs duplicados de la base de datos sin modificar"
      ],
      "metadata": {
        "id": "Ll6xOJb0hIlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean_150525 = df_final_raw_df_150525.drop_duplicates(subset=['PMID'])\n",
        "df_clean_150525.to_csv('df_clean_150525.csv', index=False)\n",
        "df_clean = pd.read_csv('df_clean_150525.csv')"
      ],
      "metadata": {
        "id": "KpMny2euhH7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agrupar palabras clave**\n",
        "\n",
        "Se crea la columna 'text_paper', donde combina las palabras clave encontradas en [Title/abstact].\n",
        "Las listas son generadas para almacenar las palabras clave de MeSH y [Title/abstract].\n",
        "Se agregan las 'STOPWORDS', filtrando signos de puntuación y partes semánticas no significativas, además se muestran en minúsculas.\n",
        "\n",
        "Recursos:\n",
        "\n",
        "(5) Stack Overflow. (2012)\n",
        "\n",
        "(6) W3Schools. (s.f.)\n",
        "\n",
        "(7) Python Software Foundation. (s.f.)\n",
        "\n",
        "(8) Kricka et al. (2021)\n",
        "\n",
        "(2) Dennstädt et al. (2024)\n",
        "\n",
        "(9) Stack Overflow (2020)"
      ],
      "metadata": {
        "id": "qmNbkbxaeApl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Se crea una nueva columna en el dF \"df_clean\" y se combinan los campos de títulos y resumen de los artículos\n",
        "df_clean['text_paper'] = df_clean['Title'] + ' ' + df_clean['Abstract']\n",
        "\n",
        "#Lista vacía para palabras clave de MeSH\n",
        "mesh_kw = []\n",
        "for keywords in df_clean['MeSH_Keywords']:\n",
        "  if keywords != 'Keyword not found':\n",
        "    words = [word.strip().lower() for word in keywords.split(',')]\n",
        "    mesh_kw.extend(words)\n",
        "\n",
        "stopwords=([\"4\", \"two\", \"2\",\"mi\", \"mrow\",\"many\", \"3\", \"may\", \"best\", \"one\", \"well\", \"three\", \"will\", \"0\", \"1\", \"S\",\"s\",\"large\",\"foundanimal\", \"founddeep\",\"synthetickeyword\",\"processingkeyword\",\"learningkeyword\", \"personnelkeyword\",\"foundkeyword\",\"intelligencekeyword\",\"proliferationkeyword\", \"foundhumans\", \"available\", \"keywords\", \"found\",\"no\", \"not\", \"keyword\"]) + list(STOPWORDS)\n",
        "\n",
        "##Lista vacía para almacenar las palabras clave de Títle/Abstract \"TiAB\"\n",
        "TiAb_kw = []\n",
        "for text_p in df_clean['text_paper']:\n",
        "  #Las palabras que se obtengan estarán en minúsculas y se eliminarán signos de puntuación\n",
        "  text_p = text_p.lower()\n",
        "  text_p = re.sub(r'[^\\w\\s]', ' ', text_p)\n",
        "  words = text_p.split()\n",
        "  words = [word for word in words if word and word not in stopwords]\n",
        "  TiAb_kw.extend(words)\n",
        "\n",
        "# Se combinan ambas listas de palabras clave\n",
        "all_kwlist = mesh_kw +  TiAb_kw"
      ],
      "metadata": {
        "id": "NZiZGltfhiPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conteo de frcuencia de palabras clave**\n",
        "\n",
        "Mediante el módulo 'Counter' de la biblioteca 'Collections', las palabras clave más comúnes dentro del conteo son analizadas\n",
        "\n",
        "Recurso:\n",
        "\n",
        "(10) GeeksforGeeks. (2025)"
      ],
      "metadata": {
        "id": "5N5wpcNtnyhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Se contean las palabras claves existentes en \"all_kwlist\"\n",
        "kw_counter = Counter(all_kwlist)\n",
        "\n",
        "#De todas las palabras clave, se tomarán sólo 77 palabras que más se repiten\n",
        "frequent_kw = kw_counter.most_common(77)\n",
        "print(\"\\nFrequent keywords:\")\n",
        "for keyword, frequency in frequent_kw:\n",
        "  print(f\"{keyword}: {frequency}\")"
      ],
      "metadata": {
        "id": "b-9ViSfKlJke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gráfico 1**\n",
        "\n",
        "La gráfica de barras contiene 15 de las palabras clave más frecuentes en MeSH y [Title/Abstract]\n",
        "\n",
        "Recursos:\n",
        "\n",
        "(11) Stack Overflow. (2022)"
      ],
      "metadata": {
        "id": "vlWsvwP9lpw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "#Se seleccionan 15 palabras más frecuentes para poderlas graficar junto con su frecuencia\n",
        "keywords = [keyword for keyword, _ in frequent_kw[:15]]\n",
        "frequencies = [frequency for _, frequency in frequent_kw[:15]]\n",
        "\n",
        "sns.barplot(x=frequencies, y=keywords, palette='Blues_d', hue=keywords, legend=False)\n",
        "plt.title('Common keywords found in Artificial Intelligence associated with Biological Sciences', fontsize=18, y=1.00)\n",
        "plt.xlabel('Frequency', fontsize=14)\n",
        "plt.ylabel('Keywords', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('common_keywords.png', dpi=300)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ARC04wyvl1gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Crear un diccionario para las palabras clave**\n",
        "\n",
        "La biblioteca de WordCloud no puede interpretar la estructura de tuplas de 'frequent_kw', por lo que se crea el diccionario donde se almacene la información como clave-valor\n"
      ],
      "metadata": {
        "id": "zhOod7ZQm7_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kw_dict = dict(frequent_kw)"
      ],
      "metadata": {
        "id": "Xc_hNoyHm7X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gráfico 2. Generar la nube de palabras**\n",
        "\n",
        "Por medio de '.generate_from_frequencies()' , el diccionario de palabras clave se agrega, de esta manera se puede conocer la frecuencia de las palabras visualmente.\n",
        "\n",
        "Recursos:\n",
        "\n",
        "(12)  Mueller. (2020)\n",
        "\n",
        "(13) Mueller. (2020)"
      ],
      "metadata": {
        "id": "G51G6wqqnQSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud_freq = WordCloud(max_words=100, stopwords=stopwords, width=1600, height=800, background_color=\"black\", max_font_size=150, collocations=True).generate_from_frequencies(kw_dict)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(wordcloud_freq.recolor(colormap='tab20b'),interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=1.08)\n",
        "plt.savefig('wordcloud_freq.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zuY4TuJ4nQBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversión de datos**\n",
        "\n",
        "Esta parte es importante para convertir la columna de 'Years' en string a numérico, de esta manera se podrá gráficar mediante la ditribución de las publicaciones a través de los años. Además los términos de la IA son cambiados a string.\n",
        "\n",
        "Recursos:\n",
        "\n",
        "(14) Stack Overflow. (2020)\n",
        "\n",
        "(15) GeeksforGeeks. (2024)"
      ],
      "metadata": {
        "id": "j3jvT80HdGPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#La columna de \"Year\" se convierte a númerico para posteriormente sean filtrados entre 2000 y 2025\n",
        "df_clean['Year']=pd.to_numeric(df_clean['Year'], errors='coerce')\n",
        "df_clean = df_clean[(df_clean['Year'] >= 2000) & (df_clean['Year'] <= 2025)]\n",
        "\n",
        "#Este paso es para examinar la distribución de los artículos por año\n",
        "year_distribution=df_clean.groupby([\"Year\"]).size()\n",
        "print(\"Distribution:\")\n",
        "print(year_distribution)\n",
        "\n",
        "#En un nuevo dataframe donde agrupan los datos como los términos de búsqueda de la IA y de biología. Y se cuentan cuántos términos existen.\n",
        "df_clean['Term_List']=df_clean['Term_List'].astype(str)\n",
        "df_bar = df_clean.groupby(['Bio_Term', 'Term_List']).size().reset_index(name='Count')\n",
        "\n",
        "print(df_clean)"
      ],
      "metadata": {
        "id": "tWAUO-18d7oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gráfico 3. Conteo de publicaciones de la IA y los campos biológicos**\n",
        "\n",
        "Desde 'df_bar', se crea una gráfica que muestra el total de publicaciones encontradas en cada combinación de los términos de búsqueda entre la IA y los campos de la biología establecidos.\n",
        "\n",
        "Recursos:\n",
        "\n",
        "(16) Python Graph Gallery. (2024)\n",
        "\n",
        "(17) Laiq, F. (2024)\n",
        "\n",
        "(18) Modin Project. (2019)\n",
        "\n",
        "(19) Mulina, V. (2023)"
      ],
      "metadata": {
        "id": "jMZSl5HgoQek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 8))\n",
        "#Para la gráfia de barras, en el eje \"x\", se agregan los campos biológicos y en el eje \"y\", se agrega el conteo.\n",
        "ax=sns.barplot(x='Bio_Term', y='Count', hue='Term_List', data=df_bar, palette='Blues_d')\n",
        "\n",
        "plt.title('Publications of Artificial Intelligence in the biological sciences (2000-2025)', fontsize=20, y= 1.05)\n",
        "plt.xlabel('Biological Field', fontsize=15)\n",
        "plt.ylabel('Count', fontsize=15)\n",
        "plt.xticks(rotation=0, fontsize=12)\n",
        "\n",
        "#Se anotan los valores a cada una de las barras\n",
        "for p in ax.patches:\n",
        "  ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=13)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('barplot_count.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jp4L6q_PoMOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gráfico 4. Menciones totales de la IA en las ciencias biológicas**\n",
        "\n",
        "Desde 'yearly_counts', para mostrar la distribución en años. En 'summary_df', 'Term_List' y 'Count' son agregados, de esta manera se identifica el uso de la IA en los campos de la biología"
      ],
      "metadata": {
        "id": "Jdc-Nz0VpHam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#En este dF, se toma \"yearly_counts\" el cuál se vuelve a reagrupar tomando sólo el año y el área de la IA\n",
        "summary_df = yearly_counts.groupby(['Year', 'Term_List'])['Count'].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='Year', y='Count', hue='Term_List', data=summary_df, markers= True, dashes=False, linewidth=2.5, palette='mako')\n",
        "\n",
        "\n",
        "plt.title(\"Total publications using Artificial Intelligence in the biological sciences (2000-2025)\", fontsize=18)\n",
        "plt.xlabel('Year', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "plt.xticks(sorted(df_clean['Year'].unique()), rotation=30)\n",
        "plt.legend(title='IA field', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"total_pub\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PEod_XRdoUa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referencias\n",
        "\n",
        "(1) Dennstädt, F., Zink, J., Putora, P. M., Hastings, J., & Cihoric, N. (2024). Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain. Systematic Reviews, 13(1). https://doi.org/10.1186/s13643-024-02575-4\n",
        "\n",
        "(2) Stack Overflow (2023). *Biopython: How can I increase the number found using Entrez.esearch?* https://stackoverflow.com/questions/75933779/biopython-how-can-i-increase-the-number-of-articles-found-using-entrez-esearch\n",
        "\n",
        "(3) TLDWTutorials. (2024). PubMed Data Extraction Script. GitHub. https://github.com/TLDWTutorials/PubmedAPI/tree/main\n",
        "\n",
        "(4) Stack Overflow. *Appending pandas dataframes generated in a for loop*. (2015). https://stackoverflow.com/questions/28669482/appending-pandas-dataframes-generated-in-a-for-loop\n",
        "\n",
        "(5) Stack Overflow. (2012). *String concatenation of two pandas columns*. https://stackoverflow.com/questions/11858472/string-concatenation-of-two-pandas-columns\n",
        "\n",
        "(6)W3Schools. (s.f.). *Python RegEx*. https://www.w3schools.com/python/python_regex.asp\n",
        "\n",
        "(7) Python Software Foundation. (s.f.). *re-Regular expression operations*. Recuperado el 22 de abril  de 2025. https://docs.python.org/3/library/re.html\n",
        "\n",
        "(8) Kricka, L. J., Cornish, T. C., & Park, J. Y. (2021). Eponyms in clinical chemistry. Clinica Chimica Acta, 512, 28–32. https://doi.org/10.1016/j.cca.2020.11.014\n",
        "\n",
        "(9) Stack Overflow (2020). *How to fund specific words in a text and count them using Python?. https://stackoverflow.com/questions/63666138/how-to-find-specific-words-in-a-text-and-count-them-using-python\n",
        "\n",
        "(10) GeeksforGeeks. (2025). *Python most_common() Function*. https://www.geeksforgeeks.org/python-most_common-function/\n",
        "\n",
        "(11) Stack Overflow. (2022). *Unpack the first two elements in list/tuple*. https://stackoverflow.com/questions/11371204/unpack-the-first-two-elements-in-list-tuple\n",
        "\n",
        "(12)  Mueller. (2020). *wordcloud.WordCoud*. https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
        "\n",
        "(13) Mueller. (2020). *Using frequency*. https://amueller.github.io/word_cloud/auto_examples/frequency.html\n",
        "\n",
        "(14) Stack Overflow. (2020). *How do I transform a pandas groupby with a condition on the transformation?*. https://stackoverflow.com/questions/61815267/how-do-i-transform-a-pandas-groupby-with-a-condition-on-the-transformation\n",
        "\n",
        "(15) GeeksforGeeks. (2024). *Pandas dataframe.groupby() Method*. https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\n",
        "\n",
        "(16) Python Graph Gallery. (2024). *Basic Barplot using Seaborn*. https://python-graph-gallery.com/basic-barplot-with-seaborn/\n",
        "\n",
        "(17) Laiq, F. (2024). *Filtrar filas que contienen una cadena específica en Pandas*. https://www.delftstack.com/es/howto/python-pandas/pandas-column-contains-string/?utm_source=chatgpt.com\n",
        "\n",
        "(18) Modin Project. (2019).*Issue #847. Plotting with seaborn*. https://github.com/modin-project/modin/issues/847\n",
        "\n",
        "(19) Mulina, V. (2023). *Bank Customers Categorization - Clustering*. https://www.kaggle.com/code/valeriamulina/bank-customers-categorization-clustering?scriptVersionId=139180478\n"
      ],
      "metadata": {
        "id": "13Sgff6O3BV2"
      }
    }
  ]
}